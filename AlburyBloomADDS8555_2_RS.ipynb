{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aaf30bae",
      "metadata": {
        "id": "aaf30bae"
      },
      "source": [
        "# DDS-8555 Assignment 2: Build Regression Models\n",
        "**Student:** Abigail Albury-Bloom  \n",
        "**Course:** DDS-8555 Predictive Analysis  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6cd4d58",
      "metadata": {
        "id": "d6cd4d58"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from math import sqrt\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "522d8f1a",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "522d8f1a"
      },
      "source": [
        "## Conceptual Question 3 (ISLR Python)\n",
        "\n",
        "**(a)** Correct answer: (iii) High-school graduates earn more on average than college graduates when GPA > 3.5.  \n",
        "**(b)** Predicted salary for a college graduate (GPA=4.0, IQ=110) = **$137,100**.  \n",
        "**(c)** Interaction term (0.01) is not negligible; statistical significance, not magnitude, determines importance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4be6de50",
      "metadata": {
        "id": "4be6de50"
      },
      "source": [
        "## Applied Question 10 (ISLR Python)\n",
        "\n",
        "**(a)** Regression model:  \n",
        "Sales = 13.04 − 0.054(Price) − 0.022(UrbanYes) + 1.20(USYes)\n",
        "\n",
        "**(b)** Price ↓ Sales (significant), Urban not significant, US ↑ Sales (significant).  \n",
        "**(c)** Qualitative variables: UrbanYes and USYes coded as 1/0.  \n",
        "**(d)** Only Price and US significant (p < 0.05).  \n",
        "**(e)** Reduced model: Sales = 13.04 − 0.054(Price) + 1.20(USYes).  \n",
        "**(f)** R² and residuals stable; adjusted R² slightly higher in reduced model.  \n",
        "**(g)** 95% CIs exclude zero for Price and US.  \n",
        "**(h)** Residuals and leverage plots show no severe violations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e788095",
      "metadata": {
        "id": "3e788095"
      },
      "source": [
        "## Kaggle Regression with Abalone Dataset\n",
        "This section uses the Kaggle *Regression with an Abalone Dataset* data to build two predictive models:\n",
        "1. **Lasso Regression** (regularized linear model)\n",
        "2. **Random Forest Regressor** (nonlinear ensemble model)\n",
        "\n",
        "Both models use an 80/20 train-validation split for RMSE evaluation, then are retrained on the full training set to create Kaggle submission files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27296ed5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "27296ed5",
        "outputId": "a9f1c5a4-1d87-4cbf-f3e2-d5a902b893a8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3166096385.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load Kaggle Abalone data (ensure train.csv and test.csv are in the same folder as this notebook)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original train columns:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load Kaggle Abalone data (ensure train.csv and test.csv are in the same folder as this notebook)\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(\"Original train columns:\", train.columns.tolist())\n",
        "\n",
        "# Drop duplicate Whole weight columns if present (keep the first 'Whole weight')\n",
        "dup_train = [c for c in train.columns if c.startswith(\"Whole weight.\")]\n",
        "dup_test = [c for c in test.columns if c.startswith(\"Whole weight.\")]\n",
        "if dup_train:\n",
        "    print(\"Dropping duplicate train columns:\", dup_train)\n",
        "    train = train.drop(columns=dup_train)\n",
        "if dup_test:\n",
        "    print(\"Dropping duplicate test columns:\", dup_test)\n",
        "    test = test.drop(columns=dup_test, errors=\"ignore\")\n",
        "\n",
        "print(\"Cleaned train columns:\", train.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c62620e",
      "metadata": {
        "id": "3c62620e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Set target and ID column robustly\n",
        "if \"Rings\" not in train.columns:\n",
        "    raise ValueError(f\"'Rings' column not found. Available columns: {train.columns.tolist()}\")\n",
        "\n",
        "target = \"Rings\"\n",
        "id_col = \"id\" if \"id\" in train.columns else None\n",
        "\n",
        "# Define features\n",
        "drop_cols = [target]\n",
        "if id_col:\n",
        "    drop_cols.append(id_col)\n",
        "\n",
        "X = train.drop(columns=drop_cols)\n",
        "y = train[target]\n",
        "\n",
        "X_test = test.drop(columns=[id_col], errors=\"ignore\") if id_col else test.copy()\n",
        "\n",
        "# Identify numeric and categorical predictors\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = [c for c in X.columns if c not in numeric_features]\n",
        "\n",
        "print(\"Numeric features:\", numeric_features)\n",
        "print(\"Categorical features:\", categorical_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f12d36b",
      "metadata": {
        "id": "9f12d36b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Preprocessing pipeline\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define models\n",
        "lasso = Lasso(alpha=0.001, max_iter=10000, random_state=42)\n",
        "rf = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Train-validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Build pipelines\n",
        "lasso_pipe = Pipeline([(\"preprocessor\", preprocessor),\n",
        "                       (\"model\", lasso)])\n",
        "\n",
        "rf_pipe = Pipeline([(\"preprocessor\", preprocessor),\n",
        "                    (\"model\", rf)])\n",
        "\n",
        "# Fit and evaluate Lasso\n",
        "lasso_pipe.fit(X_train, y_train)\n",
        "y_val_pred_lasso = lasso_pipe.predict(X_val)\n",
        "rmse_lasso = sqrt(mean_squared_error(y_val, y_val_pred_lasso))\n",
        "\n",
        "# Fit and evaluate Random Forest\n",
        "rf_pipe.fit(X_train, y_train)\n",
        "y_val_pred_rf = rf_pipe.predict(X_val)\n",
        "rmse_rf = sqrt(mean_squared_error(y_val, y_val_pred_rf))\n",
        "\n",
        "print(f\"Lasso RMSE on validation set: {rmse_lasso:.5f}\")\n",
        "print(f\"Random Forest RMSE on validation set: {rmse_rf:.5f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb8dae6",
      "metadata": {
        "id": "5fb8dae6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Retrain on full data and create Kaggle submission files\n",
        "\n",
        "# Fit on full training data\n",
        "lasso_pipe.fit(X, y)\n",
        "rf_pipe.fit(X, y)\n",
        "\n",
        "# Predict on test set\n",
        "test_pred_lasso = lasso_pipe.predict(X_test)\n",
        "test_pred_rf = rf_pipe.predict(X_test)\n",
        "\n",
        "# Build submission DataFrames\n",
        "if id_col and id_col in test.columns:\n",
        "    sub_lasso = pd.DataFrame({id_col: test[id_col], target: test_pred_lasso})\n",
        "    sub_rf = pd.DataFrame({id_col: test[id_col], target: test_pred_rf})\n",
        "else:\n",
        "    sub_lasso = pd.DataFrame({\"id\": np.arange(len(test_pred_lasso)), target: test_pred_lasso})\n",
        "    sub_rf = pd.DataFrame({\"id\": np.arange(len(test_pred_rf)), target: test_pred_rf})\n",
        "\n",
        "# Save CSVs\n",
        "sub_lasso.to_csv(\"submission_lasso.csv\", index=False)\n",
        "sub_rf.to_csv(\"submission_rf.csv\", index=False)\n",
        "\n",
        "print(\"Saved: submission_lasso.csv and submission_rf.csv\")\n",
        "sub_lasso.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Table 1. RMSE Comparison Between Lasso and Random Forest Models\n",
        "\n",
        "| Model              | RMSE   |\n",
        "|--------------------|--------|\n",
        "| Lasso Regression   | 2.1566 |\n",
        "| Random Forest      | 2.0635 |\n",
        "\n",
        "The Random Forest model achieved the lowest RMSE, indicating slightly higher predictive accuracy compared to the Lasso model.\n"
      ],
      "metadata": {
        "id": "FoZrWOdn7WIO"
      },
      "id": "FoZrWOdn7WIO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top Predictors Identified by Random Forest\n",
        "\n",
        "The Random Forest model ranked the following features as most important in predicting abalone age:\n",
        "\n",
        "1. Shell weight  \n",
        "2. Height  \n",
        "3. Diameter  \n",
        "4. Whole weight  \n",
        "5. Length  \n",
        "\n",
        "These variables contribute most to model accuracy, reflecting the strong biological relationship between physical size and the number of rings (age).\n",
        "  "
      ],
      "metadata": {
        "id": "xA9wyy8L7q0w"
      },
      "id": "xA9wyy8L7q0w"
    },
    {
      "cell_type": "markdown",
      "id": "769f3ddf",
      "metadata": {
        "id": "769f3ddf"
      },
      "source": [
        "## Regression Assumption Checks (Abalone)\n",
        "The following cells assess linear regression assumptions using an OLS model on the Abalone data:\n",
        "- Linearity and homoscedasticity via residuals vs. fitted plot\n",
        "- Normality via Q-Q plot\n",
        "- Independence via Durbin–Watson statistic\n",
        "- Multicollinearity via Variance Inflation Factor (VIF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "428b706e",
      "metadata": {
        "id": "428b706e"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Start from the original feature matrix X and target y already defined above\n",
        "\n",
        "# 1. One-hot encode categorical variables\n",
        "X_ols = pd.get_dummies(X.copy(), drop_first=True)\n",
        "\n",
        "# 2. Remove any duplicate columns just in case\n",
        "X_ols = X_ols.loc[:, ~X_ols.columns.duplicated()]\n",
        "\n",
        "# 3. Sanitize column names so nothing weird/keyword-like breaks statsmodels\n",
        "clean_cols = []\n",
        "for c in X_ols.columns:\n",
        "    # keep only letters, numbers, underscore\n",
        "    c_clean = re.sub(r'[^0-9a-zA-Z_]+', '_', c)\n",
        "    # avoid reserved name \"weights\"\n",
        "    if c_clean.lower() == \"weights\":\n",
        "        c_clean = \"weight_var\"\n",
        "    clean_cols.append(c_clean)\n",
        "X_ols.columns = clean_cols\n",
        "\n",
        "# 4. Force everything to numeric float\n",
        "X_ols = X_ols.apply(pd.to_numeric, errors=\"coerce\").astype(float)\n",
        "\n",
        "# 5. Drop any rows with NaNs created in the process, align y\n",
        "valid_rows = ~X_ols.isna().any(axis=1)\n",
        "X_ols = X_ols.loc[valid_rows, :]\n",
        "y_ols = y.loc[valid_rows].astype(float)\n",
        "\n",
        "# 6. Add constant\n",
        "X_ols_const = sm.add_constant(X_ols)\n",
        "\n",
        "# 7. Fit OLS\n",
        "ols_model = sm.OLS(y_ols, X_ols_const).fit()\n",
        "print(ols_model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19a11690",
      "metadata": {
        "id": "19a11690"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Residual diagnostics\n",
        "residuals = ols_model.resid\n",
        "fitted = ols_model.fittedvalues\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(fitted, residuals, alpha=0.5)\n",
        "plt.axhline(0, linestyle=\"--\")\n",
        "plt.xlabel(\"Fitted values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals vs Fitted\")\n",
        "plt.show()\n",
        "\n",
        "sm.qqplot(residuals, line=\"45\")\n",
        "plt.title(\"Q-Q Plot of Residuals\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Durbin-Watson statistic:\", round(durbin_watson(residuals), 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6c8307b",
      "metadata": {
        "id": "a6c8307b"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "X_vif = X_ols.copy()\n",
        "\n",
        "# Clean column names (remove symbols and reserved words)\n",
        "X_vif.columns = [re.sub(r'[^0-9a-zA-Z_]+', '_', c) for c in X_vif.columns]\n",
        "X_vif.columns = [c if c.lower() != \"weights\" else \"weight_var\" for c in X_vif.columns]\n",
        "\n",
        "# Ensure everything is numeric float and finite\n",
        "X_vif = X_vif.apply(pd.to_numeric, errors=\"coerce\").astype(float)\n",
        "X_vif = X_vif.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "# Add constant term (OLS intercept)\n",
        "X_vif_const = sm.add_constant(X_vif)\n",
        "\n",
        "# Compute VIF safely\n",
        "vif_data = pd.DataFrame({\n",
        "    \"Feature\": X_vif_const.columns,\n",
        "    \"VIF\": [variance_inflation_factor(X_vif_const.values, i)\n",
        "            for i in range(X_vif_const.shape[1])]\n",
        "})\n",
        "\n",
        "# Sort and show top results\n",
        "vif_data.sort_values(\"VIF\", ascending=False).head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GitHub Repository\n",
        "https://github.com/alburybloom/ADDS8555-2\n"
      ],
      "metadata": {
        "id": "7giY9Ruf7vrV"
      },
      "id": "7giY9Ruf7vrV"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}